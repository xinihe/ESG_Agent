{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import MarianMTModel, MarianTokenizer, PegasusForConditionalGeneration, PegasusTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载中文到英文的MarianMT翻译模型\n",
    "model_name_translation = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer_translation = MarianTokenizer.from_pretrained(model_name_translation)\n",
    "model_translation = MarianMTModel.from_pretrained(model_name_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MarianMTModel.from_pretrained(model_name_translation) is responsible for loading the model weights into memory. It will trigger a `pytorch_model.bin` file to be stored `locally` on your computer if the process completes successfully.\n",
    "\n",
    "If you are downloading a pre-trained model from Hugging Face's transformers library (or similar), the file is cached locally in your home directory `~/.cache/huggingface/transformers/`. Within this directory, subdirectories are created for each model based on its name or repository.\n",
    "\n",
    "If you specify a custom path when downloading or saving the model (e.g., model.save_pretrained('custom_path')), the file will be stored in that directory.\n",
    "\n",
    "Once downloaded, the file remains on your disk and is reused whenever you load the model again, unless you manually delete it.\n",
    "\n",
    "Managing Storage:\n",
    "\n",
    "- Clear Cache: You can clear unused models from the cache if you need to free up space:\n",
    "bash\n",
    "`huggingface-cli cache delete`\n",
    "\n",
    "- Move to External Drive: You can also move the cache directory to an external drive or another location by setting the TRANSFORMERS_CACHE environment variable:\n",
    "bash\n",
    "`export TRANSFORMERS_CACHE=/path/to/your/custom/location`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载英文总结的Pegasus模型\n",
    "model_name_summary = \"google/pegasus-xsum\"\n",
    "tokenizer_summary = PegasusTokenizer.from_pretrained(model_name_summary)\n",
    "model_summary = PegasusForConditionalGeneration.from_pretrained(model_name_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"google/pegasus-xsum\")\n",
    "print(f\"Model cached at: {model.config.architectures}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esg_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
