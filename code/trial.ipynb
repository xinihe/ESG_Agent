{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import MarianMTModel, MarianTokenizer, PegasusForConditionalGeneration, PegasusTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. model_name_translation = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "\n",
    "\tPurpose:\n",
    "\t•\tSpecifies the name of the translation model.\n",
    "\t•\t\"Helsinki-NLP/opus-mt-zh-en\" is a model from the Helsinki-NLP group in the Hugging Face Transformers library.\n",
    "\t•\topus-mt-zh-en indicates that the model is fine-tuned for translating from Chinese (zh) to English (en).\n",
    "\tWhy?:\n",
    "\t•\tThis string will be passed to Hugging Face’s from_pretrained method to load the pre-trained model and tokenizer.\n",
    "\n",
    "2. tokenizer_translation = MarianTokenizer.from_pretrained(model_name_translation)\n",
    "\n",
    "\t•\tWhat It Does:\n",
    "\t•\tInitializes a tokenizer for the MarianMT (Marian Machine Translation) model.\n",
    "\t•\tMarianTokenizer is a tokenizer designed specifically for MarianMT models. It:\n",
    "\t1.\tEncodes Input Text: Converts Chinese text into token IDs that the model understands.\n",
    "\t2.\tDecodes Output Text: Converts the model’s output token IDs back into readable English text.\n",
    "\t•\tThe from_pretrained(model_name_translation) method loads the tokenizer’s vocabulary and configuration for this specific model (Helsinki-NLP/opus-mt-zh-en) from the Hugging Face model hub.\n",
    "\t•\tWhy?:\n",
    "\t•\tTokenization is a crucial preprocessing step for any NLP model. For translation, the tokenizer ensures:\n",
    "\t•\tChinese text is tokenized in a way that aligns with the model’s training.\n",
    "\t•\tThe model can generate appropriate English text during decoding.\n",
    "\n",
    "\n",
    "3. model_translation = MarianMTModel.from_pretrained(model_name_translation)\n",
    "\n",
    "\t•\tWhat It Does:\n",
    "\t•\tLoads the pre-trained MarianMT model for Chinese-to-English translation.\n",
    "\t•\tMarianMTModel is a class specifically designed for MarianMT models in the Hugging Face Transformers library. It provides:\n",
    "\t•\tThe encoder-decoder architecture required for translation tasks.\n",
    "\t•\tThe model weights fine-tuned on Chinese-to-English translation data.\n",
    "\t•\tWhy?:\n",
    "\t•\tThe from_pretrained method ensures that the exact architecture and weights corresponding to the Helsinki-NLP/opus-mt-zh-en model are loaded, so it can perform translations accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165960f3254141499b795fc74a77fa29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b15fe1d1cf4ec38c92bbdd1e602e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/805k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1826524a269463099fec5022b882cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/807k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9a0b52d44642f7a38a77ed438cd345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f656ab49aa47c28eb230976ee71e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c34110ebbe4d19887378696aa7a5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba328e918e8041a2a5227065c7a2ee99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the model name\n",
    "model_name_translation = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer_translation = MarianTokenizer.from_pretrained(model_name_translation)\n",
    "model_translation = MarianMTModel.from_pretrained(model_name_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MarianMTModel.from_pretrained(model_name_translation) is responsible for loading the model weights into memory. It will trigger a `pytorch_model.bin` file to be stored `locally` on your computer if the process completes successfully.\n",
    "\n",
    "If you are downloading a pre-trained model from Hugging Face's transformers library (or similar), the file is cached locally in your home directory `~/.cache/huggingface/transformers/`. Within this directory, subdirectories are created for each model based on its name or repository.\n",
    "\n",
    "If you specify a custom path when downloading or saving the model (e.g., model.save_pretrained('custom_path')), the file will be stored in that directory.\n",
    "\n",
    "Once downloaded, the file remains on your disk and is reused whenever you load the model again, unless you manually delete it.\n",
    "\n",
    "Managing Storage:\n",
    "\n",
    "- Clear Cache: You can clear unused models from the cache if you need to free up space:\n",
    "bash\n",
    "`huggingface-cli cache delete`\n",
    "\n",
    "- Move to External Drive: You can also move the cache directory to an external drive or another location by setting the TRANSFORMERS_CACHE environment variable:\n",
    "bash\n",
    "`export TRANSFORMERS_CACHE=/path/to/your/custom/location`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: Hello, world!\n"
     ]
    }
   ],
   "source": [
    "# Example text to translate (Chinese)\n",
    "text_to_translate = \"你好，世界！\"  # \"Hello, world!\"\n",
    "\n",
    "# Preprocess: Tokenize the text\n",
    "input_tokens = tokenizer_translation(text_to_translate, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Translate: Generate predictions\n",
    "translated_tokens = model_translation.generate(**input_tokens)\n",
    "\n",
    "# Postprocess: Decode the output tokens to readable English\n",
    "translated_text = tokenizer_translation.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Translated text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output file paths\n",
    "input_file = \"chinese_text.txt\"  # File containing Chinese text (one line per sentence)\n",
    "output_file = \"translated_text.txt\"  # File to save the English translations\n",
    "\n",
    "# Read the input file and translate each line\n",
    "try:\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in infile:\n",
    "            # Remove leading/trailing whitespaces\n",
    "            line = line.strip()\n",
    "\n",
    "            # Skip empty lines\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Tokenize the input text\n",
    "            input_tokens = tokenizer_translation(line, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "            # Generate translation\n",
    "            translated_tokens = model_translation.generate(**input_tokens)\n",
    "\n",
    "            # Decode the translated tokens to English text\n",
    "            translated_text = tokenizer_translation.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "            # Write the translated text to the output file\n",
    "            outfile.write(translated_text + \"\\n\")\n",
    "\n",
    "    print(f\"Translation completed. Translated text saved to '{output_file}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263806a4b75440708361c2562866052a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b272a9ec041e48c699eb78495064418e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44f7b8ef0e34c51886e5ac3c6b0d39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da88e9e52b7a4949ba5ff644d2ac1d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.52M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ab5d8e806b49fb98052a20ad5e812d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f72c78b114d40cf8391f46686fa6512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607848c2bf93496a948290e5f779427c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/259 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the model name\n",
    "model_name_summary = \"google/pegasus-xsum\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer_summary = PegasusTokenizer.from_pretrained(model_name_summary)\n",
    "model_summary = PegasusForConditionalGeneration.from_pretrained(model_name_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model cached at: ['MarianMTModel']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model cached at: {model_translation.config.architectures}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esg_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
